{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GRPO Reasoning Training with Unsloth\n",
        "\n",
        "This notebook demonstrates how to train a model to \"think\" (Reasoning/Chain-of-Thought) using **GRPO (Group Relative Policy Optimization)**.\n",
        "\n",
        "We adapt a standard Instruct model (e.g., `Qwen2.5-3B-Instruct`) to output reasoning traces within `<think>` tags via Reinforcement Learning, using the **Open R1** approach.\n",
        "\n",
        "**Hardware**: Designed to run on a free **Colab T4 (16GB RAM)** instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "!pip install --upgrade -qqq uv\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    # If you're not in Colab, just use pip install!\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    # Specific versions for Colab T4 compatibility from Unsloth\n",
        "    try: import numpy, PIL; get_numpy = f\"numpy=={numpy.__version__}\"; get_pil = f\"pillow=={PIL.__version__}\"\n",
        "    except: get_numpy = \"numpy\"; get_pil = \"pillow\"\n",
        "    try: import subprocess; is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n",
        "    except: is_t4 = False\n",
        "    \n",
        "    # vLLM 0.9.2 is required for T4 to avoid 'fileno' and other issues\n",
        "    get_vllm, get_triton = (\"vllm==0.9.2\", \"triton==3.2.0\") if is_t4 else (\"vllm==0.10.2\", \"triton\")\n",
        "    !uv pip install -qqq --upgrade \\\n",
        "        unsloth {get_vllm} {get_numpy} {get_pil} torchvision bitsandbytes xformers\n",
        "    !uv pip install -qqq {get_triton}\n",
        "!uv pip install transformers==4.56.2\n",
        "!uv pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel, PatchFastRL\n",
        "from unsloth import is_bfloat16_supported\n",
        "import torch\n",
        "\n",
        "# Patch GRPO for optimizations\n",
        "PatchFastRL(\"GRPO\", FastLanguageModel)\n",
        "\n",
        "max_seq_length = 1024 # Can increase for longer reasoning chains\n",
        "lora_rank = 32 # Larger rank = smarter, but slower\n",
        "gpu_memory_utilization = 0.6 # Adjustable for T4 (0.6 is safe)\n",
        "\n",
        "# Load Model - Using Qwen2.5-3B-Instruct for speed/memory efficiency on T4\n",
        "# (You can switch to \"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\" if you have A100/L4)\n",
        "model_name = \"unsloth/Qwen2.5-3B-Instruct-bnb-4bit\"\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True,\n",
        "    fast_inference = True, # Enable vLLM fast inference\n",
        "    max_lora_rank = lora_rank,\n",
        "    gpu_memory_utilization = gpu_memory_utilization,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = lora_rank,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = lora_rank,\n",
        "    use_gradient_checkpointing = \"unsloth\", # Enable long context support\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preparation\n",
        "We use the `open-r1/DAPO-Math-17k-Processed` dataset (or GSM8K). We define a System Prompt that instructs the model to strict formatting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# System prompt to encourage thinking\n",
        "system_prompt = \"\"\"You are a helpful assistant. You are given a problem.\n",
        "You must think about the problem and provide your working out inside <think> and </think> tags.\n",
        "Then, provide the final answer.\"\"\"\n",
        "\n",
        "dataset = load_dataset(\"open-r1/DAPO-Math-17k-Processed\", split = \"train\")\n",
        "\n",
        "# Simple processing to format for GRPO\n",
        "def process_data(x):\n",
        "    return {\n",
        "        \"prompt\": [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": x[\"prompt\"]},\n",
        "        ],\n",
        "        \"answer\": x[\"solution\"], # Raw answer for verification\n",
        "    }\n",
        "\n",
        "dataset = dataset.map(process_data)\n",
        "# Filter for length to avoid OOM\n",
        "dataset = dataset.filter(lambda x: len(x[\"prompt\"][1][\"content\"]) < 500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reward Functions\n",
        "GRPO uses reward functions to guide the model. We use two:\n",
        "1. **Format Reward**: Checks if `<think>` and `</think>` tags exist.\n",
        "2. **Correctness Reward**: Checks if the answer matches the solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# 1. Format Reward: Reward for using <think> tags\n",
        "def format_reward_func(completions, **kwargs):\n",
        "    pattern = r\"<think>.*?</think>\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.search(pattern, r, re.DOTALL) for r in responses]\n",
        "    return [0.5 if match else 0.0 for match in matches]\n",
        "\n",
        "# 2. Correctness Reward: Check if the number in the response matches the solution\n",
        "# This is a simplified regex matcher for numbers\n",
        "def correctness_reward_func(prompts, completions, answer, **kwargs):\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    \n",
        "    # --- ADDED: Print Output for User ---\n",
        "    print(f\"\\n\\nGenerated Response:\\n{responses[0]}\\n\" + \"-\" * 50)\n",
        "    # ------------------------------------\n",
        "    \n",
        "    extracted_responses = []\n",
        "    \n",
        "    # Attempt to extract the last number or boxed answer\n",
        "    for r in responses:\n",
        "        # Look for content after </think>\n",
        "        parts = r.split(\"</think>\")\n",
        "        if len(parts) > 1:\n",
        "            final_part = parts[-1]\n",
        "            # Extract last number found\n",
        "            nums = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", final_part)\n",
        "            if nums:\n",
        "                extracted_responses.append(nums[-1])\n",
        "            else:\n",
        "                extracted_responses.append(None)\n",
        "        else:\n",
        "            extracted_responses.append(None)\n",
        "\n",
        "    scores = []\n",
        "    for guess, true_ans in zip(extracted_responses, answer):\n",
        "        if guess is None:\n",
        "            scores.append(0.0)\n",
        "            continue\n",
        "        \n",
        "        # Very basic fuzzy match\n",
        "        try:\n",
        "            if abs(float(guess) - float(true_ans)) < 1e-6:\n",
        "                scores.append(1.0)\n",
        "            else:\n",
        "                scores.append(0.0)\n",
        "        except:\n",
        "            scores.append(0.0)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trl import GRPOConfig, GRPOTrainer\n",
        "\n",
        "training_args = GRPOConfig(\n",
        "    output_dir = \"grpo_output\",\n",
        "    run_name = \"grpo_thinking_run\",\n",
        "    learning_rate = 5e-6,\n",
        "    adam_beta1 = 0.9,\n",
        "    adam_beta2 = 0.99,\n",
        "    weight_decay = 0.1,\n",
        "    warmup_ratio = 0.1,\n",
        "    lr_scheduler_type = \"cosine\",\n",
        "    logging_steps = 1,\n",
        "    bf16 = is_bfloat16_supported(),\n",
        "    fp16 = not is_bfloat16_supported(),\n",
        "    per_device_train_batch_size = 1,\n",
        "    gradient_accumulation_steps = 4,\n",
        "    num_generations = 4, # How many attempts per prompt (group size)\n",
        "    max_prompt_length = 256,\n",
        "    max_completion_length = 200, # Limit generation length for speed\n",
        "    max_steps = 250, # Keep it short for demo\n",
        "    save_steps = 50,\n",
        "    max_grad_norm = 0.1,\n",
        "    report_to = \"none\",\n",
        "    use_vllm = True, # Use vLLM for fast generation\n",
        "    vllm_gpu_memory_utilization = 0.3, # Reserved for vLLM\n",
        ")\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    reward_funcs = [format_reward_func, correctness_reward_func],\n",
        "    args = training_args,\n",
        "    train_dataset = dataset,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the model\n",
        "text = tokenizer.apply_chat_template([\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\", \"content\": \"What is 15 * 7?\"}\n",
        "], tokenize = False, add_generation_prompt = True)\n",
        "\n",
        "from vllm import SamplingParams\n",
        "sampling_params = SamplingParams(\n",
        "    temperature = 0.8,\n",
        "    top_p = 0.95,\n",
        "    max_tokens = 1024,\n",
        ")\n",
        "\n",
        "output = model.fast_generate(\n",
        "    [text],\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = None,\n",
        ")[0].outputs[0].text\n",
        "\n",
        "print(output)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}